{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is one of the notebooks I ran in parallel to get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T18:30:47.817743Z",
     "start_time": "2018-03-01T18:30:47.814089Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Begin and End Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T18:30:49.275140Z",
     "start_time": "2018-03-01T18:30:49.273032Z"
    }
   },
   "outputs": [],
   "source": [
    "begindate = 19810101   # Webpage layout if is different before this date\n",
    "enddate = 20180301"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Data to gather:\n",
    "- 'web_url'\n",
    "\n",
    "- 'pub_date'\n",
    "- 'snippet'\n",
    "- 'source'\n",
    "- 'word_count'\n",
    "- 'type_of_material' (News,OpEd, etc.)\n",
    "- 'document_type' (article, multimedia, etc.)\n",
    "- 'main' (webiste title)\n",
    "- 'print_headline' (print title)\n",
    "- 'person' (writer name)\n",
    "- 'section_name'\n",
    "- 'score' (no clue what this means yet)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Metadata through API, add to Articles Dict, and Export to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T05:01:27.163631Z",
     "start_time": "2018-03-01T05:01:26.834774Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    begin_date = 19810101\n",
    "    end_date = 20160812\n",
    "    articles = {}\n",
    "    exportno = 3\n",
    "    page = 0\n",
    "    while page < 201 and exportno < 8:\n",
    "        if len(articles)>=1000:\n",
    "            print('Exporting results till page {} to json {}'.format(page-1,exportno))\n",
    "            with open('exports/metadeta_export{}.json'.format(exportno), 'w') as fp:\n",
    "                json.dump(articles, fp)\n",
    "            articles = {}\n",
    "            exportno += 1\n",
    "            \n",
    "#         print('Getting articles from page {}'.format(page))\n",
    "        \n",
    "        url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "        params = {'api-key': '...',\n",
    "                  'q': 'mental%20health',\n",
    "                  'begin_date': begin_date, \n",
    "                  'end_date': end_date,\n",
    "                  'sort':'newest',\n",
    "                  'page': page}\n",
    "        response = requests.get(url, params=params)\n",
    "#         print('status code: {}'.format(response.status_code))\n",
    "        \n",
    "        time.sleep(0.6)\n",
    "        \n",
    "        while response.status_code != 200:\n",
    "            print('trying again...')\n",
    "            time.sleep(3)\n",
    "            response = requests.get(url, params=params)\n",
    "            print('status code: {}'.format(response.status_code))\n",
    "        \n",
    "        articlesjson = json.loads(response.text)\n",
    "        \n",
    "        docs = articlesjson['response']['docs']\n",
    "        \n",
    "        try:\n",
    "            for i in range(0,len(docs)):\n",
    "                item = docs[i]\n",
    "                \n",
    "                # Get URL\n",
    "                articles[item['web_url']] = {}\n",
    "                \n",
    "                # Get Name of Writer\n",
    "                try:\n",
    "                    articles[item['web_url']]['writer_name'] = item['byline']['person'][0]['firstname']+' '+item['byline']['person'][0]['lastname']\n",
    "                except:\n",
    "                    articles[item['web_url']]['writer_name'] = None\n",
    "\n",
    "                # Get Publication Date\n",
    "                try:\n",
    "                    articles[item['web_url']]['pub_date'] = item['pub_date'][:10]\n",
    "                    tempdate = ''.join((item['pub_date'][:10]).split('-'))\n",
    "                except:\n",
    "                    articles[item['web_url']]['pub_date'] = None\n",
    "                \n",
    "                # Get Snippet\n",
    "                try:\n",
    "                    articles[item['web_url']]['snippet'] = item['snippet']\n",
    "                except:\n",
    "                    articles[item['web_url']]['snippet'] = None\n",
    "                \n",
    "                # Get Word Count\n",
    "                try:\n",
    "                    articles[item['web_url']]['word_count'] = item['word_count']\n",
    "                except:\n",
    "                    articles[item['web_url']]['word_count'] = None\n",
    "                    \n",
    "                 # Get Score\n",
    "                try:\n",
    "                    articles[item['web_url']]['score'] = item['score']\n",
    "                except:\n",
    "                    articles[item['web_url']]['score'] = None\n",
    "               \n",
    "                # Get Source\n",
    "                try:\n",
    "                    articles[item['web_url']]['source'] = item['source']\n",
    "                except:\n",
    "                    articles[item['web_url']]['source'] = None\n",
    "                    \n",
    "                # Get Section Name\n",
    "                try:\n",
    "                    articles[item['web_url']]['section_name'] = item['section_name']\n",
    "                except:\n",
    "                    articles[item['web_url']]['section_name'] = None\n",
    "               \n",
    "                # Get Type of Material\n",
    "                try:\n",
    "                    articles[item['web_url']]['type_of_material'] = item['type_of_material']\n",
    "                except:\n",
    "                    articles[item['web_url']]['type_of_material'] = None\n",
    "                \n",
    "                # Get Document Type\n",
    "                try:\n",
    "                    articles[item['web_url']]['document_type'] = item['document_type']\n",
    "                except:\n",
    "                    articles[item['web_url']]['document_type'] = None\n",
    "                \n",
    "                # Get Main / Web Headline\n",
    "                try:\n",
    "                    articles[item['web_url']]['main_headline'] = item['headline']['main']\n",
    "                except:\n",
    "                    articles[item['web_url']]['main_headline'] = None\n",
    "               \n",
    "                # Get Print Headline\n",
    "                try:\n",
    "                    articles[item['web_url']]['print_headline'] = item['headline']['print_headline']\n",
    "                except:\n",
    "                    articles[item['web_url']]['print_headline'] = None   \n",
    "                    \n",
    "            page += 1\n",
    "            \n",
    "            if page%10==0 and page>0:\n",
    "                print('Get articles page {} success'.format(page))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('ERROR:, {}'.format(e))\n",
    "            return {}\n",
    "    \n",
    "        if page == 200:\n",
    "            end_date = int(tempdate)\n",
    "            page = 0\n",
    "    \n",
    "    print('Exporting remainder including page {} to json {}'.format(page,exportno))\n",
    "    with open('exports/metadeta_export{}.json'.format(exportno), 'w') as fp:\n",
    "        json.dump(articles, fp)\n",
    "\n",
    "#     return articles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T05:11:29.426509Z",
     "start_time": "2018-03-01T05:01:27.640643Z"
    }
   },
   "source": [
    "get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Bodytext Scraping, add to Articles Dict, and Export Full Set to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T20:22:26.915527Z",
     "start_time": "2018-03-01T20:22:26.890328Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bodytext():\n",
    "    for i in np.arange(1,9):\n",
    "        print('{}: importing metadeta_export{}.json'.format(datetime.datetime.now(),i))\n",
    "        with open('exports/metadeta_export{}.json'.format(i)) as fp:\n",
    "            articles = json.load(fp)\n",
    "        for url in articles.keys():\n",
    "#             print(url)\n",
    "            page = requests.get(url)\n",
    "#             time.sleep(0.2)\n",
    "            soup = BeautifulSoup(page.text, 'lxml')\n",
    "            text = soup.findAll(attrs={'class':'story-body-text story-content'})\n",
    "            if text == []:\n",
    "                text = soup.findAll(attrs={'class':'story-body-text'})\n",
    "            if text == []:\n",
    "                text = soup.findAll(attrs={'itemprop':'articleBody'})\n",
    "            if text == []:\n",
    "                text = soup.findAll(attrs={'itemprop':'reviewBody'})\n",
    "            body_text = ''\n",
    "            for paragraph in text:\n",
    "                body_text += (' **********'+paragraph.get_text())\n",
    "            articles[url]['body_text'] = body_text\n",
    "#             time.sleep(0.6)\n",
    "        # Write to .json\n",
    "        print('{}: exporting to bodytext_export{}.json'.format(datetime.datetime.now(),i))\n",
    "        with open('exports/bodytext_export{}.json'.format(i), 'w') as fp:\n",
    "            json.dump(articles, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T21:12:44.912523Z",
     "start_time": "2018-03-01T20:22:29.337610Z"
    },
    "scrolled": false
   },
   "source": [
    "get_bodytext()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
